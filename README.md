# CUDA-GPU

GPUs' SIMT (Single Instruction Multiple Threads) model 

CUDA unlocks massive parallelism on NVIDIA GPUs for tasks like AI training and simulations, building on your prior self-taught progress in fundamentals and kernels.



<img width="305" height="130" alt="image" src="https://github.com/user-attachments/assets/213ab064-0b0f-405c-a284-31ffeef7351d" />


Cache memory is a small, ultra-fast, volatile computer memory (typically SRAM) that acts as a buffer between the CPU and the main memory (RAM). It stores frequently accessed data and instructions to significantly reduce data retrieval time, enhancing overall system performance. Modern CPUs use three levels (L1, L2, L3) of cache to minimize access delays. 

---

CPU has cache memory due to it variety in performing tasks and its complex control logic and decison making


GPU DOES have cache.


But much less (per core) compared to CPU, and it is designed differently. **To fit more cores and maximize throughput**


<img width="294" height="115" alt="image" src="https://github.com/user-attachments/assets/cbc35bff-13c1-4073-a815-8e75a417debf" />

L1,L2 and L3 Cache in CPU

<img width="1198" height="270" alt="image" src="https://github.com/user-attachments/assets/3e9315c5-948d-440e-953e-bbe128bb0028" />



<img width="592" height="625" alt="image" src="https://github.com/user-attachments/assets/764ee431-04ff-46e3-b892-15bbf53a5ed1" />


<img width="465" height="365" alt="image" src="https://github.com/user-attachments/assets/9f6bd20a-c4fe-412d-be34-bac3cce93c4c" />





----
<img width="318" height="591" alt="image" src="https://github.com/user-attachments/assets/a593c07a-8d31-4cdc-9d37-dd0f3b94c1d6" />
